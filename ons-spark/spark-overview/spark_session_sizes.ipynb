{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"http://np2rvlapxx507/DAP_CATS/guidance/-/raw/master/images/dap_cats_logo_small.png\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Sample Spark Session Sizes\n",
    "\n",
    "This document gives some example Spark sessions. For more information on Spark sessions and why you need to be careful with memory usage, please consult the [Guidance on Spark Sessions](http://np2rvlapxx507/DAP_CATS/guidance/-/blob/master/Spark%20session%20guidance.md).\n",
    "\n",
    "Remember to only use a Spark session for as long as you need. It's good etiquette to use `spark.stop()` (for PySpark) or `spark_disconnect(sc)` (for sparklyr) in your scripts. Stopping the CDSW or Jupyter Notebook session will also close the Spark session if one is running.\n",
    "\n",
    "\n",
    "### Default/Blank Session\n",
    "\n",
    "As a starting point you can create a Spark session with all the default options. This is the bare minimum you need to create a Spark session and will work fine for many DAP users.\n",
    "\n",
    "Please use this session by default or if unsure in any way about your resource requirements.\n",
    "\n",
    "Note that for PySpark, `.config(\"spark.ui.showConsoleProgress\", \"false\")` is still recommended for use with this session; this will stop the console progress in Spark, which sometimes obscures results from displaying properly.\n",
    "\n",
    "Details:\n",
    "- Will give you the default config options\n",
    "    \n",
    "Use case:\n",
    "- When unsure of your requirements\n",
    "    \n",
    "Example of actual usage:\n",
    "- Investigation of new or unfamiliar data sources\n",
    "- Building a new pipeline where full user requirements aren't yet known\n",
    "\n",
    "#### PySpark Default/Blank Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"default-session\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparklyr Default/Blank session\n",
    "\n",
    "```\n",
    "library(sparklyr)\n",
    "\n",
    "default_config <- spark_config()\n",
    "\n",
    "sc <- spark_connect(\n",
    "  master = \"yarn-client\",\n",
    "  app_name = \"default-session\",\n",
    "  config = default_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Session\n",
    "\n",
    "This is the smallest session that will realistically be used in DAP. It is similar in size to that used for DAP CATS training, as the low memory and only one core means several people can run this session at the same time on Dev Test, the cluster with the smallest capacity.\n",
    "\n",
    "Details:\n",
    "- Only 1g of memory and 3 executors\n",
    "- Only 1 core\n",
    "- Number of partitions are limited to 12, which can improve performance with smaller data\n",
    "\n",
    "Use case:\n",
    "- Simple data exploration of small survey data\n",
    "- Training and demonstrations when several people need to run Spark sessions simultaneously\n",
    "\n",
    "Example of actual usage:\n",
    "- Used for DAPCATS PySpark training, with mostly simple calculations on small data\n",
    "\n",
    "#### PySpark Small Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"small-session\")\n",
    "    .config(\"spark.executor.memory\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", 1)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 12)\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparklyr Default/Blank session\n",
    "\n",
    "```\n",
    "library(sparklyr)\n",
    "\n",
    "small_config <- spark_config()\n",
    "small_config$spark.executor.memory <- \"1g\"\n",
    "small_config$spark.executor.cores <- 1\n",
    "small_config$spark.dynamicAllocation.enabled <- \"true\"\n",
    "small_config$spark.dynamicAllocation.maxExecutors <- 3\n",
    "small_config$spark.sql.shuffle.partitions <- 12\n",
    "small_config$spark.shuffle.service.enabled <- \"true\"\n",
    "\n",
    "sc <- spark_connect(\n",
    "  master = \"yarn-client\",\n",
    "  app_name = \"small-session\",\n",
    "  config = small_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium Session\n",
    "\n",
    "A standard session used for analysing survey or synthetic datasets. Also used for some Production pipelines based on survey and/or smaller administrative data.\n",
    "\n",
    "Details:\n",
    "- 6g of memory and 3 executors\n",
    "- 3 cores\n",
    "- Number of partitions are limited to 18, which can improve performance with smaller data\n",
    "\n",
    "Use case:\n",
    "- Developing code in Dev Test\n",
    "- Data exploration in Production\n",
    "- Developing Production pipelines on a sample of data\n",
    "- Running smaller Production pipelines on mostly survey data\n",
    "\n",
    "Example of actual usage:\n",
    "- Complex calculations, but on smaller synthetic data in Dev Test\n",
    "\n",
    "#### PySpark Medium Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "        SparkSession.builder.appName(\"medium-session\")\n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .config(\"spark.executor.cores\", 3)\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", 3)\n",
    "        .config(\"spark.sql.shuffle.partitions\", 18)\n",
    "        .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparklyr Medium Session\n",
    "\n",
    "```\n",
    "library(sparklyr)\n",
    "\n",
    "medium_config <- spark_config()\n",
    "medium_config$spark.executor.memory <- \"6g\"\n",
    "medium_config$spark.executor.cores <- 3\n",
    "medium_config$spark.dynamicAllocation.enabled <- \"true\"\n",
    "medium_config$spark.dynamicAllocation.maxExecutors <- 3\n",
    "medium_config$spark.sql.shuffle.partitions <- 18\n",
    "medium_config$spark.shuffle.service.enabled <- \"true\"\n",
    "\n",
    "sc <- spark_connect(\n",
    "  master = \"yarn-client\",\n",
    "  app_name = \"medium-session\",\n",
    "  config = medium_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Session\n",
    "\n",
    "Session designed for running Production pipelines on large administrative data, rather than just survey data. Will often develop using a sample and a smaller session then change to this once the pipeline is complete.\n",
    "\n",
    "Details:\n",
    "- 10g of memory and 5 executors\n",
    "- 1g of memory overhead\n",
    "- 5 cores, which is generally optimal on larger sessions\n",
    "- The default number of 200 partitions\n",
    "    \n",
    "Use case:\n",
    "- Production pipelines on administrative data\n",
    "- Cannot be used in Dev Test, as it exceeds the 9 GB limit per executor\n",
    "\n",
    "Example of actual usage:\n",
    "- One administrative dataset of 100 million rows\n",
    "- Many calculations\n",
    "\n",
    "#### PySpark Large Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"large-session\")\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"1g\")\n",
    "    .config(\"spark.executor.cores\", 5)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 5)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 200)\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparklyr Large Session\n",
    "\n",
    "```\n",
    "library(sparklyr)\n",
    "\n",
    "large_config <- spark_config()\n",
    "large_config$spark.executor.memory <- \"10g\"\n",
    "large_config$spark.yarn.executor.memoryOverhead <- \"1g\"\n",
    "large_config$spark.executor.cores <- 5\n",
    "large_config$spark.dynamicAllocation.enabled <- \"true\"\n",
    "large_config$spark.dynamicAllocation.maxExecutors <- 5\n",
    "large_config$spark.sql.shuffle.partitions <- 200\n",
    "large_config$spark.shuffle.service.enabled <- \"true\"\n",
    "\n",
    "sc <- spark_connect(\n",
    "  master = \"yarn-client\",\n",
    "  app_name = \"large-session\",\n",
    "  config = large_config)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Large session\n",
    "\n",
    "Used for the most complex pipelines, with huge administrative data sources and complex calculations. This uses a large amount of resource on the cluster, so only use when running Production pipelines.\n",
    "\n",
    "It is even more important when using more resource to close your Spark session once finished.\n",
    "\n",
    "Details:\n",
    "- 20g of memory and 12 executors\n",
    "- 2g of memory overhead\n",
    "- 5 cores; using too many cores can actually cause worse performance on larger sessions\n",
    "- 240 partitions; not significantly higher than the default of 200, but it is best for these to be a multiple of cores and executors\n",
    "\n",
    "Use case:\n",
    "- Running large, complex pipelines in Production on mostly administrative data\n",
    "- Do not use for development purposes; use a smaller session and work on a sample of data or synthetic data\n",
    "\n",
    "Example of actual usage:\n",
    "- Three administrative datasets of around 300 million rows\n",
    "- Significant calculations, including joins and writing and reading to many intermediate tables\n",
    "\n",
    "#### PySpark Extra Large Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"xl-session\")\n",
    "    .config(\"spark.executor.memory\", \"20g\")\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"2g\")\n",
    "    .config(\"spark.executor.cores\", 5)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", 12)\n",
    "    .config(\"spark.sql.shuffle.partitions\", 240)\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparklyr Extra Large Session\n",
    "\n",
    "```\n",
    "library(sparklyr)\n",
    "\n",
    "xl_config <- spark_config()\n",
    "xl_config$spark.executor.memory <- \"20g\"\n",
    "xl_config$spark.yarn.executor.memoryOverhead <- \"2g\"\n",
    "xl_config$spark.executor.cores <- 5\n",
    "xl_config$spark.dynamicAllocation.enabled <- \"true\"\n",
    "xl_config$spark.dynamicAllocation.maxExecutors <- 12\n",
    "xl_config$spark.sql.shuffle.partitions <- 240\n",
    "xl_config$spark.shuffle.service.enabled <- \"true\"\n",
    "\n",
    "sc <- spark_connect(\n",
    "  master = \"yarn-client\",\n",
    "  app_name = \"xl-session\",\n",
    "  config = xl_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Spark sessions\n",
    "\n",
    "Remember to close your sessions once finished, so that the memory can be re-allocated to another user.\n",
    "\n",
    "#### Closing a session in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closing a session in sparklyr\n",
    "\n",
    "```\n",
    "spark_disconnect(sc)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
