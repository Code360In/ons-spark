{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "3. New images in images directory / check R Spark UI\n",
    "4. References to other pages\n",
    "5. sort out exercises\n",
    "6. How does one get to Spark UI on GCP\n",
    "\n",
    "PR:\n",
    "Check style of R code to be consistent, check package referencing in R code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Application and UI\n",
    "\n",
    "This notebook introduces the structure of a Spark application and how the structure relates to the various pages within the application monitoring interface, the Spark UI. \n",
    "\n",
    "The Spark UI is used to monitor the status and resource consumption of your Spark cluster and is the main tool for troubleshooting inefficient Spark code. \n",
    "\n",
    "Understanding the structure of a Spark application helps to understand how Spark works so you can begin to think differently as you code to get the most out of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Application Overview\n",
    "A Spark application has an underlining structure and learning about this structure will help us\n",
    "1. understand the difference between a narrow transformation, wide transformation and an action, \n",
    "2. navigate around the Spark UI\n",
    "\n",
    "Below is a diagram of the Spark application hierarchy.\n",
    "\n",
    "- **Application** - a set of jobs managed by a single driver, e.g. on a Cloudera platform that is the CDSW session. An application is created when we connect to Spark with a `SparkSession` in PySpark or `spark_connect()` in sparklyr.\n",
    "- **Job** - a set of stages executed as a result of an *action*, e.g. `.count()`/`sdf_nrow()`. A job might consist of a number of *transformations*, but a job always finishes with an *action*.\n",
    "- **Stage** - a set of tasks that can be executed in parallel. Similar to how *actions* define a job boundary, a stage boundary is created by a *wide* transformation, such as `.groupby()`/`group_by()` or `.join()`/`left_join()`. A stage might consist of many *narrow* transformations, but a stage always finishes with a *wide* transformation.\n",
    "- **Task** - an individual unit of work assigned to a single core on an executor.\n",
    "\n",
    "\n",
    "![Diagram of Spark application showing partitioned DataFrame going through narrow and wide operations](../../images/spark_app_diagram.png)\n",
    "\n",
    "\n",
    "Note that Jobs, Stages and Tasks are numbered starting from zero.\n",
    "\n",
    "Now look more closely at the DataFrame. On the left there is a Spark DataFrame which is split up into four sections, called *partitions*. Spark puts our data into partitions automatically so we don't have to decide how or when to do it. More details on exactly how Spark does this are in the **Partitions Notebook**. \n",
    "\n",
    "The first operation to be applied to this DataFrame is a *narrow* transformation, e.g. summing two columns to create a new column. The second operation is again a narrow transformation. These two operations can be run in parallel, because each child partition has only one parent, indicated by the golden arrows.\n",
    "\n",
    "Then we hit a stage boundary, which means the third operation is a *wide* transformation, e.g. grouping the data or joining with another DataFrame. This is a wide operation because each child partition has multiple parent partitions. This is also an example of *shuffling* the data, more on shuffling in the **Shuffling Notebook**.\n",
    "\n",
    "There is then another narrow operation on the grouped DataFrame before we run into an *action*, e.g. writing the data to HDFS or a simple count of the rows. \n",
    "\n",
    "As we execute the wide and narrow transformations nothing happens to the data at this point, Spark just builds up a plan of what to do with the DataFrame. This plan is a called the execution plan and is set of instructions of how to transform the data from one state to another. An action initiates the execution plan and so this is when the DataFrame is processed. All the previous transformations, along with the action, are put into Spark *jobs* and deployed on the Spark cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the Spark UI\n",
    "\n",
    "We will create an application and execute some code to create jobs. Then we can drill down from Job to Stage to Task and investigate their performance. This is generally how we would diagnose an inefficient Spark application.\n",
    "\n",
    "### Create an application\n",
    "\n",
    "The first step is to create an application. Once we create a Spark application we can look at the Spark UI. If we stop the session using `spark.stop()`/`spark_disconnect()` or stopping the CDSW session, the Spark UI for our application can no longer be accessed.\n",
    "\n",
    "Let's start with the necesarry imports and create an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, IPython\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"spark-app-ui\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(sparklyr)\n",
    "\n",
    "default_config <- sparklyr::spark_config()\n",
    "\n",
    "sc <- sparklyr::spark_connect(\n",
    "    master = \"local[2]\",\n",
    "    app_name = \"spark-app-ui\",\n",
    "    config = default_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a handy button for us to access the Spark UI. Make sure you hold down Ctrl when you click on the link to open the page in a new tab. See the [Cloudera documentation](https://docs.cloudera.com/documentation/data-science-workbench/1-0-x/topics/cdsw_spark_ui.html) for more informaiton on this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=http://spark-m94sz4rszj4av8gm.cdswmn-d01-01.ons.statistics.gov.uk>Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"spark-%s.%s\" % (os.environ[\"CDSW_ENGINE_ID\"], os.environ[\"CDSW_DOMAIN\"])\n",
    "IPython.display.HTML(\"<a href=http://%s>Spark UI</a>\" % url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "library(\"cdsw\")\n",
    "url = paste0(\"spark-\", Sys.getenv(\"CDSW_ENGINE_ID\"), \".\", Sys.getenv(\"CDSW_DOMAIN\"), sep=\"\")\n",
    "html(paste0(\"<a href=http://\", url, \">Spark UI</a>\", sep=\"\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you follow the above link you will see something silimar to the screenshot below, with the application name on the top right and various tabs along the top. \n",
    "\n",
    "There is lots of useful information in the Spark UI, but for this notebook we will only concentrate on the *Jobs* and *Stages* tabs. Note that we haven't executed any *Jobs* yet, so there isn't much to see at the moment.\n",
    "\n",
    "![Empty Spark UI page](../../images/spark_app_empty_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the animal rescue data and find out the number of partitions in which our `rescue` DataFrame is processed. The number of partitions will be useful for later when we're investigating our application's tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  2\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../../config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "rescue_path = config[\"rescue_path\"]\n",
    "rescue = spark.read.parquet(rescue_path)\n",
    "\n",
    "print(\"Number of partitions: \",rescue.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "config <- yaml::yaml.load_file(\"ons-spark/config.yaml\")\n",
    "\n",
    "rescue <- sparklyr::spark_read_parquet(sc, config$rescue_path)\n",
    "\n",
    "print(paste0(\"Number of partitions: \", sparklyr::sdf_num_partitions(population)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll carry out some basic processes to see how this translates into jobs, stages and tasks in the Spark UI. We will create a new column to group the incident costs by creating a new column called `cost_group` containing three groups `small`, `medium` and `large`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------------------------------------------+----------+----------+\n",
      "|animal_group|                                                     description|total_cost|cost_group|\n",
      "+------------+----------------------------------------------------------------+----------+----------+\n",
      "|         Cat|                                         CAT TRAPPED IN BASEMENT|     290.0|     small|\n",
      "|       Horse|                                           HORSE TRAPPED IN GATE|     590.0|    medium|\n",
      "|        Bird|PIGEON WITH WING IMAPLED ON SHARP IMPLEMENT  UNDER A BRIDGE NEAR|     326.0|    medium|\n",
      "|         Cat|                          ASSIST RSPCA WITH CAT STUCK ON CHIMNEY|     295.0|     small|\n",
      "|         Dog|                                       DOG FALLEN INTO THE CANAL|     260.0|     small|\n",
      "|        Deer|                                          DEER STUCK IN RAILINGS|     520.0|    medium|\n",
      "|        Deer|                                           DEER TRAPPED IN FENCE|     260.0|     small|\n",
      "|         Cat|                                      KITTEN TRAPPED BEHIND BATH|     333.0|    medium|\n",
      "|         Fox|                     ASSIST RSPCA WAS FOX TRAPPED IN BARBED WIRE|     298.0|     small|\n",
      "|         Cat|                                         CAT STUCK IN CAR ENGINE|    1160.0|     large|\n",
      "+------------+----------------------------------------------------------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescue = (\n",
    "    rescue.withColumn(\"cost_group\",\n",
    "                      F.when(F.col(\"total_cost\")<300, \"small\")\n",
    "                      .when((F.col(\"total_cost\")>=300) & (F.col(\"total_cost\")<900), \"medium\")\n",
    "                      .when(F.col(\"total_cost\")>=1000, \"large\")\n",
    "                      .otherwise(None)\n",
    "                     )\n",
    ")\n",
    "\n",
    "rescue.select(\"animal_group\", \"description\", \"total_cost\", \"cost_group\").show(10, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue <- rescue %>% dplyr::mutate(\n",
    "    cost_group=dplyr::case_when(total_cost<300 ~ \"small\",\n",
    "                                total_cost>=300 & total_cost<900 ~ \"medium\",\n",
    "                                total_cost>=900 ~ \"large\",\n",
    "                                TRUE ~ NULL)\n",
    ")\n",
    "\n",
    "\n",
    "rescue %>% \n",
    "    dplyr::select(\"animal_group\", \"description\", \"total_cost\", \"cost_group\") %>%\n",
    "    head(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the above `.show()`/`head()` Spark didn't need to process all of the partitions, there are enough rows on one partition to create the output so Spark only processed one partition. If we want Spark to process all of the partitions we need to call an action that involves all rows of the DataFrame, such as `.count()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescue.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "rescue %>% sparklyr::sdf_nrow()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Event Timeline\n",
    "Now that we've created some jobs let's have a quick look at the application's *Event timeline*. \n",
    "\n",
    "1. Go into the Spark UI\n",
    "2. Make sure you're on the Jobs tab\n",
    "3. Open the *Event timeline* collapsable section using the blue arrow to see something similar to the below image.\n",
    "\n",
    "Note the images in this notebook were created using PySpark, the Spark UI will look slightly different when using sparklyr, but not very different.\n",
    "\n",
    "In the top section of the timeline you will see the executors being added and removed by the *dynamic allocation* feature of Spark. The `SparkSession` used in this case was a local session, so we will only see the driver in our case. The jobs are shown in the bottom section of the timeline. Hover over a job in the timeline and the corresponding job in the *Completed Jobs* table below will be highlighted.\n",
    "\n",
    "*Tip:* You can tick the *Enable Zooming* button to zoom in and out of different sections of the timeline\n",
    "\n",
    "![Event timeline within Spark UI showing executors being assigned and jobs being deployed](../../images/spark_app_event_timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job details: stage information\n",
    "Let's look at the *Completed Jobs* table. The description gives us a clue as to the action that initiated that job. The first job, *Job Id* 0, was to interact with HDFS, with a description `parquet at NativeMethodAccessorImpl.java:0`. Spark needs to know the DataFrame's schema, i.e. column names and types, to validate our PySpark/sparklyr code. This will always create a job, usually consisting of just one stage as shown in the *Stages* column.\n",
    "\n",
    "The second job was initiated by `.show()`/`head()`, again it consists of one stage, which itself had one task as shown in the *Tasks* column. As mentioned in the previous section, Spark only needed to process one partition to produce the output so we therefore have one task. \n",
    "\n",
    "The third job was the `.count()`/`sdf_nrow()`. The job contains two stages, which themselves consist of 3 tasks. Let's get more information on these stages and tasks. Within the *Completed Jobs* table click on the link in the *Description* column for the latest job that says `count at NativeMethodAccessorImpl.java:0`. \n",
    "\n",
    "Now we are on the *Job Details* page which show information about the job's stages. The first stage had two tasks, one task to count the rows in each partition. The second stage had one task, which was to send the result to the driver for us to see. On this page you will see another type of diagram called a Directed Acyclic Graph or DAG, by opening the *DAG Vizualisation* collapsable section.\n",
    "\n",
    "The DAG shows the two stages. Here are some rough definitions of the terms inside the boxes\n",
    "1. *WholeStageCodegen*- this is Spark creating an optimised version of our code\n",
    "2. *Exchange* - another word for *shuffling* data, i.e. data is being moved between partitions\n",
    "3. *mapPartitionsInternal* - bringing information from multiple partitions together\n",
    "\n",
    "![Stage information in Spark UI showing DAG and stage table](../../images/spark_app_stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage details: task information\n",
    "\n",
    "From the *Completed Stages* table on the *Job Details* page, click on the link in the *Description* column for the stage consisting of two tasks (*Stage Id* 2 in the example above). \n",
    "\n",
    "Here you will find the *Stage Details* page that contains lots of detailed information about the individual tasks and is one of the more useful places to look for troubleshooting sluggish applications. We will just concentrate on two sections within this page, the *Event timeline* (for tasks) and the *Summary Metrics*.\n",
    "\n",
    "#### Task Event timeline\n",
    "\n",
    "Open the *Event timeline* collapseable menu to see the below image. The timeline has a single row, which means one executor was used to complete these tasks. The executor ID in this case was `cdhwn-d01-06.ons.statistics.gov.uk`. This executor has a single core, we know this because the *Executor Computing Time* (shown in green) for the first task had finished before the second task started. If there were two cores on this executor the two tasks could have ran in parallel, at the same time.\n",
    "\n",
    "![Task event timeline in Spark UI showing parallel processing on tasks](../../images/spark_app_task_timeline.png)\n",
    "\n",
    "The colours also indicate what was going on while the task was being completed. In general- green is good, and is an indication that there is no need worry or spend time optimising the processing. Delays often occur when lots of data are moved around, because the process can involve some or all of the below:\n",
    "1. serialising data - preparing the data to be moved to disk\n",
    "2. shuffle write - writing data to disk to be read by a shuffle somewhere else\n",
    "3. shuffle read - reading a shuffle from disk onto an executor or driver\n",
    "4. deserialisation - preparing the data to be read in memory\n",
    "\n",
    "Sometimes when processing many small partitions more time is spent on moving small amounts of data around than useful processing time. This task event timeline will show evidence of this problem in the form of non-green colours. We'll see this in action later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Metrics\n",
    "\n",
    "Summary information about the tasks within a stage are given in the Summary Metrics table. Select the *(De)select All* option to view more metrics.\n",
    "\n",
    "This is a useful indication of the distribution of times taken for various components of task processing. It might not be the most useful part of the UI to look at with only two tasks, so we will revisit this later.\n",
    "\n",
    "![Task metrics table in the Spark UI](../../images/spark_app_task_metrics.png)\n",
    "\n",
    "We won't discuss *GC time*, or garbage collection time, in this notebook. This is a topic that is covered in **a separate article**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "### Documentation\n",
    "\n",
    "There are tens of blogs available online that introduce some feature in the Spark UI, but most are not particularly useful and are generally rehashes of other blogs. \n",
    "\n",
    "Finally, since its release of version 3.0.0, Spark's online documentation contains information about the [Spark web UI](https://spark.apache.org/docs/latest/web-ui.html). Note however that this is a later version of Spark than what is currently used in DAP at the ONS. Therefore there will be some small differences between what you see in your application's UI and that documented in the above link. \n",
    "\n",
    "The documentation runs through the different pages of the Spark UI with screenshots and a brief description of the various elements. It's a good place to start and useful for defining terminology. For example, if you want to know what *Scheduler delay* means, search for this term on the docs page and you will find, \n",
    "\n",
    ">**Scheduler delay** is the time the task waits to be scheduled for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving data to the driver\n",
    "\n",
    "Note that in our case using the Data Access Platform (DAP) we could call the *driver* here *CDSW session*, but we'll use driver to be consistent with other material in this book.\n",
    "\n",
    "What can the Spark UI tell us about processes that run on the driver? Let's investigate by \n",
    "1. aggregate the `population` DataFrame\n",
    "2. use `.toPandas()`/`collect()` to move the data to the driver\n",
    "3. make a plot of the aggregated data\n",
    "4. check the Spark UI\n",
    "\n",
    "First step is to aggregate the data, we'll find the count of incidents in each `cost_group`. We will distinguish between the Spark and Pandas DataFrames using `_spark` or `_pandas` as suffixes respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost_group</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medium</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>small</td>\n",
       "      <td>3691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cost_group  count\n",
       "0       None     82\n",
       "1     medium   2045\n",
       "2      small   3691\n",
       "3      large     80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_spark = rescue.groupBy(\"cost_group\").agg(F.count(\"incident_number\").alias(\"count\"))\n",
    "\n",
    "#import pandas as pd\n",
    "aggregated_pandas = aggregated_spark.toPandas()\n",
    "\n",
    "aggregated_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "aggregated_spark <- rescue %>% \n",
    "    dplyr::group_by(cost_group) %>%\n",
    "    dplyr::summarise(count=n())\n",
    "\n",
    "aggregated_r <- aggregated_spark %>% sparklyr::collect()\n",
    "\n",
    "aggregated_r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f09ae065c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_pandas = aggregated_pandas.set_index(\"cost_group\").loc[[\"small\", \"medium\", \"large\", \"None\"]]\n",
    "aggregated_pandas.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "aggregated_r <- aggregated_r %>% dplyr::arrange(desc(cost_group))\n",
    "\n",
    "ggplot2::ggplot(aggregated_r, ggplot2::aes(cost_group, count)) +\n",
    "    ggplot2::geom_bar(stat=\"identity\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our chart, let's see how that translates to tasks in the Spark UI.\n",
    "\n",
    "There is a job for `.toPandas()`/`collect()`, but nothing after that for the plot. Why? \n",
    "\n",
    "The plotting was done in Pandas/R and so Spark was not involved at all. We therefore don't expect the Spark UI to show anything that represents those processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving performance\n",
    "\n",
    "Finally, let's take a look at the stages and tasks for the latest job in the Spark UI and have and see an example of identifying a performance issue, how to solving it, an viewing evidence of improvement.\n",
    "\n",
    "The `.toPandas()`/`collect()` job has two stages because the `.groupBy().agg()`/`group_by()%>%summarise()` command is a wide transformation. In the first stage the data is read in, a new column added and then the dataset is aggregated. This stage has 2 tasks because the `rescue` DataFrame has 2 partitions. The second stage in this job moves the data from the cluster to the CDSW session. This stage has 200 tasks, because `aggregated_spark` has 200 partitions, more on why this is the case in the **Partitions notebook**. A screenshot of the *Stage details* for these tasks is below.\n",
    "\n",
    "![Task information for 200 tasks](../../images/spark_app_200_tasks.png)\n",
    "\n",
    "As we're running in local mode with 2 cores (or threads), the processing of these 200 tasks took place on the driver. The tasks seem to show relatively poor performance, which is indicated by the non-green colours in the timeline. Also by comparing the *Duration* (useful processing time) metrics with that of *Scheduler Delay* and *Task Deserialization Time* in the *Summary Metrics* table. Would we get better performance if the `aggregated_spark` DataFrame had fewer partitions? Let's try to improve the performace by reducing the number of partitions from 200 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_spark = rescue.groupBy(\"cost_group\").agg(F.count(\"incident_number\").alias(\"count\")).coalesce(2)\n",
    "aggregated_pandas = aggregated_spark.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```r\n",
    "aggregated_spark <- rescue %>% \n",
    "    dplyr::group_by(cost_group) %>%\n",
    "    dplyr::summarise(count=n()) %>%\n",
    "    sparklyr::sdf_coalesce(2)\n",
    "\n",
    "                                    \n",
    "aggregated_r <- aggregated_spark %>% \n",
    "    sparklyr::collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now navigate to the details of the latter stage of the latest job within the UI and you will see something similar to the screenshot below. Of course, the all important metric to compare is the time taken to complete the stage. Previously, using 200 partitions the stage time was 0.6 seconds, using 2 partitions the stage time was 0.3 seconds. Looking at this page we can see why.\n",
    "\n",
    "![Task timeline for fewer tasks showing better performance](../../images/spark_app_2_tasks.png)\n",
    "\n",
    "Again, the processing took place on the driver with 2 cores, but this time there was one task per core. There's much more green visible in this timeline and comparing *Scheduler Delay* and *Task Deserialization Time* with *Duration* in the *Summary Metrics* tells a very different story.\n",
    "\n",
    "The important point is that we are processing a small amount of data and therefore should reduce the partitioning. With 200 partitions a lot of time was spent scheduling tasks and (de)serializing data. By putting our small DataFrame into fewer partitions we spent more time on useful processing and decareased the overall processing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Spark application hierarchy**\n",
    "\n",
    "    - Application \n",
    "        - Job \n",
    "            - Stage \n",
    "                - Task\n",
    "\n",
    "\n",
    "**Use the Spark UI to**\n",
    "- debug slow or crashing applications\n",
    "- investigate what Spark is doing \"under the hood\"\n",
    "\n",
    "\n",
    "**Tips**\n",
    "- Look out for task colours, green is generally good\n",
    "- Use online documentation for more information about the UI\n",
    "- Lots of small partitions is an inefficient strategy to process data\n",
    "- Matching up the executed code with the job number in the UI is difficult, the description starts with the action used to initiate that job. You can also customise the **job description**(http://np2rvlapxx507/DAP_CATS/troubleshooting/tip-of-the-week/blob/master/tip_14_setJobDescription.ipynb) to track jobs easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. In the [Task Event timeline](#Task-Event-timeline) section it was stated that \"if each executor within this Spark session had two cores the two tasks would run in parallel\". Test this. \n",
    "\n",
    "    Stop the Spark session using `spark.stop()` and re-run this notebook but this time with two cores on each executor. \n",
    "        a) Is the above statement true? \n",
    "        b) Did this stage take less time with the extra core? \n",
    "        c) Would increasing the cores from two to three per executor improve performance?\n",
    "    \n",
    "    *Tip: When running the notebook a second time run all the cells in order to get the same stage number (Stage 3)*\n",
    "    \n",
    "\n",
    "2. We showed that Spark DataFrames are monitored by the Spark UI but Pandas DataFrames are not. What about Koalas? If you haven't heard of koalas have a look at [this guidance on the ONS Data Service Hub](https://officenationalstatistics.sharepoint.com/sites/DatCapDatSer/SitePages/Koalas%20comparison.aspx). \n",
    "\n",
    "    Have a think about it first. Then import koalas as demonstrated in the above link, convert the Spark DataFrame to Koalas using,\n",
    "    \n",
    "    `sex_age_group_count_koalas = ks.DataFrame(sex_age_group_count_spark)`\n",
    "    \n",
    "    and create `age_group_count_by_sex_koalas` using the same syntax as was used to create `age_group_count_by_sex_pandas` (remember to apply `.pivot_table()` to the Koalas DataFrame as opposed to the Spark DataFrame).\n",
    "    \n",
    "    **Bonus question** on this exercise - does the ordering work? i.e. `.loc[age_order]` or `.loc[age_order[::-1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "\n",
    "# Step 1\n",
    "# spark.stop() to stop the current Spark session\n",
    "\n",
    "# Step 2\n",
    "# Change .config(\"spark.executor.cores\", 1) to .config(\"spark.executor.cores\", 2) within the SparkSession.builder\n",
    "\n",
    "# Step 3\n",
    "# Run the whole notebook again\n",
    "\n",
    "# Step 4\n",
    "# Check the task Event timeline for Stage 3 in the Spark UI\n",
    "# a) Yes! It did work. \n",
    "# b) As shown in the notebook, Stage 3 using one core took 0.6s.\n",
    "#    Running the notebook with two cores gave 0.4s for Stage 3.\n",
    "#    Note that due to the nature of distributed computing you might get different answers each time you run the notebook.\n",
    "# c) Assuming the people DataFrame consists of two partitions, \n",
    "#    increasing cores from two to three per executor would **not** improve performance for this stage. \n",
    "#    Two partitions translates to two tasks for this stage, therefore there would be one redundant core. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=0.15 and pyspark<3.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex        female   male\n",
      "age_group               \n",
      "<30         732.0  742.0\n",
      ">60         965.0  990.0\n",
      "30-60       779.0  792.0\n",
      "\n",
      "Using .loc[] to modify order:\n",
      "sex        female   male\n",
      "age_group               \n",
      "<30         732.0  742.0\n",
      ">60         965.0  990.0\n",
      "30-60       779.0  792.0\n",
      "\n",
      "Looks like using .loc[] to specify order does not work with Koalas\n"
     ]
    }
   ],
   "source": [
    "# Solution 2\n",
    "import databricks.koalas as ks\n",
    "\n",
    "sex_age_group_count_koalas = ks.DataFrame(sex_age_group_count_spark)\n",
    "\n",
    "age_group_count_by_sex_koalas = sex_age_group_count_koalas.pivot_table(\n",
    "    columns=\"sex\", index=[\"age_group\"], values=(\"count\")\n",
    ").loc[age_order[::-1]]\n",
    "\n",
    "print(age_group_count_by_sex_koalas)\n",
    "\n",
    "print(\"\\nUsing .loc[] to modify order:\")\n",
    "\n",
    "print(age_group_count_by_sex_koalas.loc[age_order])\n",
    "\n",
    "print(\"\\nLooks like using .loc[] to specify order does not work with Koalas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
